{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/LukeDitria/pytorch_tutorials/blob/main/section02_pytorch_basics/notebooks/Tutorial1_Pytorch_Basics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFunction, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * 2 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_function = SimpleFunction()\n",
    "# perform the forward pass\n",
    "output = simple_function(10)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the input x and returns the output x * w^t + b\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.w = nn.Parameter(torch.randn(output_size, input_size))\n",
    "        self.b = nn.Parameter(torch.randn(1, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.w.t()) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.randn(10, 5)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearModel(5, 1)\n",
    "output = linear_model(input_data)\n",
    "\n",
    "print(output.shape)\n",
    "print(output.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytorch inbuilt Neural Network Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear layer aka a \"fully connected\" layer aka a \"Perceptron\" layer\n",
    "# nn.Linear(Number of inputs, Number of outputs) \n",
    "linear = nn.Linear(3, 1) \n",
    "\n",
    "# Lets have a look at the parameters of this layer\n",
    "# The \"weights\" are what is multipied by the input data\n",
    "print ('w:\\n', linear.weight.data)\n",
    "# The bias is then added on!\n",
    "print ('b:\\n', linear.bias.data)\n",
    "\n",
    "print ('w shape:\\n', linear.weight.data.shape)\n",
    "print ('b shape:\\n', linear.bias.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the gradients of these parameters\n",
    "print ('w:\\n', linear.weight.grad)\n",
    "print ('b:\\n', linear.bias.grad)\n",
    "# Note: Pytorch initialises the grad of the tensors to \"None\" NOT 0!\n",
    "# They only get created after the first backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random data input tensor\n",
    "data = torch.randn(100, 3)\n",
    "# Create some noisey target data\n",
    "target = data.sum(1, keepdims=True) + 0.01*torch.randn(data.shape[0], 1)\n",
    "print ('Input data:\\n', data[:10])\n",
    "print ('Output data:\\n', target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember! To perform a forward pass of our model, we just need to \"call\" our network\n",
    "# Pytorch's nn.Module class will automatically pass it to the \"forward\" function in the layer class\n",
    "target_pred = linear(data)\n",
    "print(\"Network output:\\n\", target_pred.data[:10])\n",
    "print(\"Network output shape:\", target_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Functions and Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets perform a regression with a mean square error loss\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Lets create a Stochastic gradient descent optimizer with a learning rate of 0.01\n",
    "# (the way we will be using it, it is just normal GD) \n",
    "# When we create the optimizer we need to tell it WHAT it needs to optimize, so the first thing \n",
    "# We pass it are the linear layer's \"parameters\"\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first dimension of the input vs the output\n",
    "\n",
    "# Use the outputs of the model from a few cells ago\n",
    "plt.scatter(data[:, 0], target_pred.detach())\n",
    "# Use the Ground Truth data\n",
    "plt.scatter(data[:, 0], target, marker=\"x\")\n",
    "plt.legend([\"Predictions\", \"Ground Truth Data\"])\n",
    "plt.xlabel(\"Inputs\")\n",
    "plt.ylabel(\"Ouputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function(target_pred, target)\n",
    "print('loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass.\n",
    "loss.backward()\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "# Note for every backwards pass of the model we must first perform a forward pass\n",
    "# as data from parts of the computational graph have been deleted upon the backward pass to save memory.\n",
    "# We can tell Pytorch to hold onto this data, but, in many cases it needs to be recalculated anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "\n",
    "# Perform another forward pass of the model to check the new loss\n",
    "target_pred = linear(data)\n",
    "loss = loss_function(target_pred, target)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Training Loop**\n",
    "For MOST tasks (but not all) a simgle training iteration in Pytorch can be summarised in the following 5 steps:\n",
    "- Forward pass of our model with the data.\n",
    "- Calculate the loss.\n",
    "- Reset the current stored gradients to 0\n",
    "- Backpropagate the loss to calculate the new gradients.\n",
    "- Perform an optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logger = []\n",
    "\n",
    "for i in range(1000):\n",
    "    # Perform a forward pass of our data\n",
    "    target_pred = linear(data)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = loss_function(target_pred, target)\n",
    "    \n",
    "    # .zero_grad sets the stored gradients to 0\n",
    "    # If we didn't do this they would be added to the \n",
    "    # Gradients from the previous step!\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate the new gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perform an optimization step!\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_logger.append(loss.item())\n",
    "    \n",
    "print(\"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first dimension of the input vs the output\n",
    "plt.scatter(data[:, 0], target_pred.detach())\n",
    "plt.scatter(data[:, 0], target, marker=\"x\")\n",
    "plt.legend([\"Predictions\", \"Ground Truth\"])\n",
    "plt.xlabel(\"Inputs\")\n",
    "plt.ylabel(\"Ouputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
